{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "import urllib.request\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from random import uniform\n",
    "import pandas as pd\n",
    "# chromedriver로 크롬 사이트 접속하기\n",
    "# 드라이버 경로는 본인 컴퓨터 경로로 수정해야함\n",
    "driver = webdriver.Chrome('./chromedriver_win32/chromedriver.exe')\n",
    "driver.implicitly_wait(3)\n",
    "# 네이버 쇼핑윈도 - 원피스 카테고리로 들어가기\n",
    "driver.get('https://search.shopping.naver.com/search/category.nhn?pagingIndex=1&pagingSize=40&productSet=window&viewType=list&sort=rel&cat_id=50000807&frm=NVSHCAT')\n",
    "# csv 파일 가져오기\n",
    "import csv\n",
    "file='./0_1999.csv'#파일이름\n",
    "f = open(file,'rt')\n",
    "link_addr_file = csv.reader(f)\n",
    "link_list=[]\n",
    "for line in link_addr_file:\n",
    "    link_list.append(line[1])\n",
    "f.close()    \n",
    "len(link_list)\n",
    "link_list\n",
    "#상세 페이지 들어가서 원하는 정보 저장\n",
    "data_list = pd.DataFrame()\n",
    "#link_list 에서 범위 조정 가능\n",
    "for i in link_list:    \n",
    "    driver.get(i)\n",
    "    print(i)\n",
    "    \n",
    "    f = uniform(1.5, 1.7)\n",
    "    time.sleep(f)\n",
    "\n",
    "    a = \"\"\n",
    "    a = i.split(\"_mid=\")\n",
    "    a[1][0:11]\n",
    "    link = i\n",
    "\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser') \n",
    "\n",
    "    try:\n",
    "        images = soup.select('div._396uGDX0Fi > img')\n",
    "        name = soup.find('h3','_2IA5sp7BRM').get_text()\n",
    "        fit = driver.find_element_by_css_selector('div._1Hbih69XFT > div:nth-child(4) > table > tbody > tr:nth-child(1) > td:nth-child(2)').text        \n",
    "        sleeve = driver.find_element_by_css_selector('div._1Hbih69XFT > div:nth-child(4) > table > tbody > tr:nth-child(2) > td:nth-child(2)').text\n",
    "        length = driver.find_element_by_css_selector('div._1Hbih69XFT > div:nth-child(4) > table > tbody > tr:nth-child(3) > td:nth-child(2)').text\n",
    "        pattern = driver.find_element_by_css_selector('div._1Hbih69XFT > div:nth-child(4) > table > tbody > tr:nth-child(4) > td:nth-child(2)').text\n",
    "        material = driver.find_element_by_css_selector('div._1Hbih69XFT > div:nth-child(4) > table > tbody > tr:nth-child(1) > td:nth-child(4)').text\n",
    "        neckline = driver.find_element_by_css_selector('div._1Hbih69XFT > div:nth-child(4) > table > tbody > tr:nth-child(2) > td:nth-child(4)').text\n",
    "        skirt = driver.find_element_by_css_selector('div._1Hbih69XFT > div:nth-child(4) > table > tbody > tr:nth-child(3) > td:nth-child(4)').text\n",
    "        detail = driver.find_element_by_css_selector('div._1Hbih69XFT > div:nth-child(4) > table > tbody > tr:nth-child(4) > td:nth-child(4)').text\n",
    "        \n",
    "#         price = soup.find('span','_1dpDPg-OQb').get_text()\n",
    "#         tag=soup.find('div','_24FKuaBdoK').get_text()\n",
    "#         info = [soup.find_all('th','_15qeGNn6Dt')[n].string for n in range(0,20)]\n",
    "#         detail = [soup.find_all('td','jvlKiI0U_y')[n].string for n in range(0,20)]\n",
    "#         texts = soup.find_all('p', 'se_textarea')\n",
    "\n",
    "        if not os.path.isdir(a[1][0:11]):\n",
    "            os.mkdir(a[1][0:11])     \n",
    "        for image in images:\n",
    "\n",
    "            src_page = requests.get(image['src'], headers={\"Referer\":\"https://www.naver.com/\"})\n",
    "            src_image = src_page.content\n",
    "            with open(\".\\\\\"+a[1][0:11]+\"\\\\img_\"+a[1][0:11]+'_'+str(images.index(image))+\".png\", \"wb\") as downfile:\n",
    "                downfile.write(src_image)\n",
    "            f = uniform(0.4, 0.6)    \n",
    "            time.sleep(f)\n",
    "#         for text in texts:\n",
    "#             text_list.append(text.get_text().strip().replace(u'\\xa0', u' '))\n",
    "#             text_list2 = [zz for zz in text_list if zz]\n",
    "\n",
    "        # 긁어올 카테고리 명을 입력하세요     \n",
    "        data_1 = {'name': [name], 'link': [link], 'fit': [fit], 'sleeve': [sleeve], 'length': [length], 'pattern': [pattern], \n",
    "                  'material': [material], 'neckline': [neckline], 'skirt': [skirt], 'detail': [detail], 'imagefile':[\"img_\"+a[1][0:11]+'_'+str(images.index(image))+\".png\"]}\n",
    "\n",
    "    \n",
    "#         data_1 = ({'name': [name], 'link': [link], 'fit': [fit], 'sleeve': [sleeve], 'length': [length],\n",
    "#                   'pattern': [pattern], 'material': [material], 'neckline': [neckline], 'skirt': [skirt], 'detail': [detail]})\n",
    "        df1 = pd.DataFrame(data_1)\n",
    "        \n",
    "#         data_2 = [detail]\n",
    "#         df2 = pd.DataFrame(data_2, columns = info)\n",
    "\n",
    "        data_list= pd.concat([data_list,df1], axis=0)\n",
    "\n",
    "        df1.to_csv(\".\\\\\"+a[1][0:11]+\"\\\\csv_\"+a[1][0:11]+\".csv\", index = True, encoding='utf-8-sig')\n",
    "    except:\n",
    "        print(\"상품정보 가져오기 오류발생\")\n",
    "        pass\n",
    "data_list.to_csv(file[0:-3]+\"_list.csv\", index=True, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
